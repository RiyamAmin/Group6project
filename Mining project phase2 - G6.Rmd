---
title: "lung cancer patients"
output: html_notebook
---
# Data Mining Project - Group 6

## 1- Intruduction:
In this lung cancer data mining project, our goal is to analyze the dataset and find the key risk factors associated with the disease. To achieve this, we will employ various data mining techniques including data preprocessing, data analysis, classification methods, and clustering.

## 2- Problem:
In this lung cancer data mining project, our goal is to analyze the dataset and find the key risk factors associated with the disease. To achieve this, we will employ various data mining techniques including data preprocessing, data analysis, classification methods, and clustering. Our dataset contains information on patients with lung cancer, including their age, gender, air pollution exposure, alcohol use, dust allergy, occupational hazards, genetic risk, chronic lung disease, balanced diet, obesity, smoking, passive smoker, chest pain, coughing of blood, fatigue, weight loss, shortness of breath, wheezing, swallowing difficulty, clubbing of finger nails and snoring.

## 3- Goal:
The goal of collecting this dataset is to analyze it and study the main reasons of lung cancer and how to treat it.

## 4- Sources:
Dataset source: https://www.kaggle.com/datasets/thedevastator/cancer-patients-and-air-pollution-a-new-link?resource=download

## 5- Dataset description:
|Attribute name                    |Description                                                |Data type   |
|------------------|------------------|------------------|
|Age                               |The age of the patient.                                    |Numeric     |
|Gender	                           |The gender of the patient.                                 |Ordinal     |
|Air Pollution	                   |The level of air pollution exposure of the patient.        |Ordinal     |
|Alcohol use	                     |The level of alcohol use of the patient.                   |Ordinal     |
|Dust Allergy                      |The level of dust allergy of the patient.                  |Ordinal     |
|OccuPational Hazards	             |The level of occupational hazards of the patient.          |Ordinal     |
|Genetic Risk	                     |The level of genetic risk of the patient.                  |Ordinal     |
|Chronic Lung Disease	             |The level of chronic lung disease of the patient.          |Ordinal     |
|Balanced Diet	                   |The level of balanced diet of the patient.                 |Ordinal     |
|Obesity	                         |The level of obesity of the patient.                       |Ordinal     |
|Smoking	                         |The level of smoking of the patient.                       |Ordinal     |
|Passive Smoker	                   |The level of passive smoker of the patient.                |Ordinal     |
|Chest Pain	                       |The level of chest pain of the patient.                    |Ordinal     |
|Coughing of Blood	               |The level of coughing of blood of the patient.             |Ordinal     |
|Fatigue	                         |The level of fatigue of the patient.                       |Ordinal     |
|Weight Loss	                     |The level of weight loss of the patient.                   |Ordinal     |
|Shortness of Breath	             |The level of shortness of breath of the patient.           |Ordinal     |
|Wheezing	                         |The level of wheezing of the patient.                      |Ordinal     |
|Swallowing Difficulty	           |The level of swallowing difficulty of the patient.         |Ordinal     |
|Clubbing of Finger Nails          |The level of clubbing of finger nails of the patient.      |Ordinal     |

## General information:
Number of attributes: 26 
Attributes type: Nominal, binary, numeric 
Number of objects: 1000 

#### Class label: Level


## 6- Importing data

```{r}
setwd("~/IT326 Project")
data <- read.csv("cancer patient data sets.csv")
View(data)
### We imported the dataset from our working directory.

str(data)
### Our dataset has 26 attributes and 1000 observations.
### Also, every attribue of the dataset is an integer, Patient.Id and Level.
### Patient.Id is nominal, Age is numiric, Gender is binary and the rest are ordinal.
```


```{r}
is.na(data)
### The dataset has no missing values.
```


### Sample:
```{R}
library(dplyr)
samp <- data[, c(2,3,4,9,12, 13, 26)]
mySample <- sample_n(samp, 50)
print(mySample)
### We selected 50 observations from the attributes 2,3,4,9,12,13 and 26 as a sample for our dataset.
```


########################


## 7- statistical measurements 
### Riham

```{R}
summary(data$Age)
### calculating the min, 1st quartile, Median, mean, 3rd quartile, and the max of the age attribute.

range(data$Age)
### The ages of the patients falls in the range of 14-73 years old.

var(data$Age)
sd(data$Age)
### We calculated the variance of the age attribute which is 144.13, thus the standard devaition is indeed around 12.
### These values imply that the data is dispersed and is far from the mean, due to the outliers that need to be smoothed later on.
```

```{R}
level_factor <- factor(data$Level,  levels = c("Low","Medium","High"), ordered = TRUE)
print(level_factor)
level_factor <- as.numeric(level_factor)
### We can't calculate correlation or covariance on non-numeric data, so we converted the Level attribute to an ordered factor, then to a numeric vector.
```

```{R}
cor(level_factor, data$Age)
### Correlation of Level  and age attributes is positive hence, the attributes are correlated.

cor(level_factor, data$Gender)
### There is a negative correlation between the gender of the patients and the Level of lung cancer, so these variables are independent, removing Gender attributes is preferred.

cor(level_factor, data$Genetic.Risk)
### There is a strong positive correlation between the Level of lung cancer and the genetic risk, so these variables are dependent.

cov(level_factor, data$Obesity)
### Covariance is greater than 1, meaning that Level of lung cancer and the Obesity have a positive linear relationship.

cov(level_factor, data$Smoking)
### Covariance is greater than 1, meaning that Level of lung cancer and Smoking have a positive linear relationship.
```


######################



## 8- graphs and tables
### Ashwaq & Ruaa

```{r}
hist(data$Smoking)
### The Histogram shows the level frequency of Smoking in patients.
### we noticed that the highest frequency was when the level of smoking of the patient is within the range [1-3] and [7] and it illustrates there is a lot of outliers in the dataset.
```


```{r}
hist(data$Alcohol.use)
### The Histogram shows the level frequency of alcohol use in patients.
### we noticed that the highest frequency was when the level Alcohol use within this range [1-2] and [7-8].
```

```{r}
hist(data$Air.Pollution)
### The Histogram shows the level frequency of Air pollution in patients.
### we noticed that the highest frequency was when the level Air pollution in the 6 and it illustrates there is a lot of outliers in the dataset.
```

```{r}
data$Gender %>% table() %>% pie()
### add percentages
tab <- data$Gender %>% table()
precentages <- tab %>% prop.table() %>% round(3) * 100 
txt <- paste0(names(tab), '\n', precentages, '%') # text on chart
pie(tab, labels=txt) # plot pie chart

### The pie chart shows the Gender of patients.
```

```{r}
boxplot(data$Age)
### The age boxplot shows that the average age of the patients lies between 30 and 40 years, in addition to an outlier in age attribute.
```

```{r}
plot(data$Dry.Cough, data$Smoking)
### The scatter plot shows that there is no relation between Dry cough and Smoking.
```


```{r}
plot(data$Dust.Allergy, data$Chest.Pain)
### The scatter plot shows that there is no relation between Dust Allergy and Chest pain.
```

```{r}
plot(data$Air.Pollution, data$Chest.Pain)
### The scatter plot shows that there is no relation between Air pollution and Chest pain.
```

```{r}
boxplot(data$Air.Pollution ~ data$Level)
### The boxplot show there is outliers and Most of the values fall in the medium range and there is no relation between increasing the level of Air pollution and increasing level of lung cancer.
```

```{r}
boxplot(data$Dust.Allergy ~ data$Level)
### The boxplot show there is outliers and Most of the values fall in the medium range and there is a relation between increasing the level Dust Allergy and increasing level lung cancer.
```

```{r}
boxplot(data$Coughing.of.Blood ~ data$Level)
### The boxplot shows that most of the values fall in the low range and there is a relation between increasing the level coughing of blood and increasing level lung cancer, there is outliers too which needs to be smoothed.
```

```{r}
boxplot(data$Fatigue ~ data$Level)
### The boxplot show there is no outliers and Most of the values fall in the High range and there is a positive relation between increasing the level fatigue and increasing level of lung cancer.
```

```{r}
boxplot(data$Shortness.of.Breath ~ data$Level)
### The boxplot show there is an outlier and most of the values fall in the medium range and there is a positive relation between increasing the level shortness of breath and increasing level of lung cancer.
```

```{r}
boxplot(data$Wheezing ~ data$Level)
### The boxplot show there is outliers and Most of the values fall in the high range and there is a relation between the level wheezing and the level of lung cancer.
```

###################

## 9- Data cleaning
### Joud 
```{r}
data <- data[, c(2,3,5:26)]
### Removed the index attribute, we already have Patient Id attribute.
### Removed The Gender attribute since there is a negative correlation between it and the class label.
```

### Detecting Outliers
### TRUE represents outlier, FALSE represents non-outlier

```{r}
library(outliers)

OutAge = outlier(data$Age, logical = TRUE)
sum(OutAge)
Find_outlier1 = which(OutAge == TRUE, arr.ind = TRUE)
OutAge
Find_outlier1
data = data[-Find_outlier1,]
### Smoothing outliers when present.

### Repeat process to smooth all outliers in the dataset.
```
```{r}
OutAirP = outlier(data$Air.Pollution, logical = TRUE)
sum(OutAirP)
Find_outlier2 = which(OutAirP == TRUE, arr.ind = TRUE)
OutAirP
Find_outlier2
data = data[-Find_outlier2,]

OutAlcohol = outlier(data$Alcohol.use, logical = TRUE)
sum(OutAlcohol)
Find_outlier3 = which(OutAlcohol == TRUE, arr.ind = TRUE)
OutAlcohol
Find_outlier3
data = data[-Find_outlier3,]

OutDust = outlier(data$Dust.Allergy, logical = TRUE)
sum(OutDust)
Find_outlier4 = which(OutDust == TRUE, arr.ind = TRUE)
OutDust
Find_outlier4
data = data[-Find_outlier4,]

OutOccu = outlier(data$OccuPational.Hazards, logical = TRUE)
sum(OutOccu)
Find_outlier5 = which(OutOccu == TRUE, arr.ind = TRUE)
OutOccu
Find_outlier5
data = data[-Find_outlier5,]

OutGenetic = outlier(data$Genetic.Risk, logical = TRUE)
sum(OutGenetic)
Find_outlier6 = which(OutGenetic == TRUE, arr.ind = TRUE)
OutGenetic
Find_outlier6
data = data[-Find_outlier6,]

OutChroncDisease = outlier(data$chronic.Lung.Disease, logical = TRUE)
sum(OutChroncDisease)
Find_outlier7 = which(OutChroncDisease == TRUE, arr.ind = TRUE)
OutChroncDisease
Find_outlier7
data = data[-Find_outlier7,]

OutDiet = outlier(data$Balanced.Diet, logical = TRUE)
sum(OutDiet)
Find_outlier8 = which(OutDiet == TRUE, arr.ind = TRUE)
OutDiet
Find_outlier8
data = data[-Find_outlier8,]

OutObesity = outlier(data$Obesity, logical = TRUE)
sum(OutObesity)
Find_outlier9 = which(OutObesity == TRUE, arr.ind = TRUE)
OutObesity
Find_outlier9
data = data[-Find_outlier9,]

OutSmoking = outlier(data$Smoking, logical = TRUE)
sum(OutSmoking)
Find_outlier10 = which(OutSmoking == TRUE, arr.ind = TRUE)
OutSmoking
Find_outlier10
data = data[-Find_outlier10,]

OutPassiveS= outlier(data$Passive.Smoker, logical = TRUE)
sum(OutPassiveS)
Find_outlier11 = which(OutPassiveS == TRUE, arr.ind = TRUE)
OutPassiveS
Find_outlier11
data = data[-Find_outlier11,]

OutChestPain = outlier(data$Chest.Pain, logical = TRUE)
sum(OutChestPain)
Find_outlier12 = which(OutChestPain == TRUE, arr.ind = TRUE)
OutChestPain
Find_outlier12
data = data[-Find_outlier12,]

OutBloodCoughing = outlier(data$Coughing.of.Blood, logical = TRUE)
sum(OutBloodCoughing)
Find_outlier13 = which(OutBloodCoughing == TRUE, arr.ind = TRUE)
OutBloodCoughing
Find_outlier13
data = data[-Find_outlier13,]

OutFatigue = outlier(data$Fatigue, logical = TRUE)
sum(OutFatigue)
Find_outlier14 = which(OutFatigue == TRUE, arr.ind = TRUE)
OutFatigue
Find_outlier14
data = data[-Find_outlier14,]

OutWeightLoss = outlier(data$Weight.Loss, logical = TRUE)
sum(OutWeightLoss)
Find_outlier15 = which(OutWeightLoss == TRUE, arr.ind = TRUE)
OutWeightLoss
Find_outlier15
data = data[-Find_outlier15,]

OutBreathShortness = outlier(data$Shortness.of.Breath, logical = TRUE)
sum(OutBreathShortness)
Find_outlier16 = which(OutBreathShortness == TRUE, arr.ind = TRUE)
OutBreathShortness
Find_outlier16
data = data[-Find_outlier16,]

OutWheezing = outlier(data$Wheezing, logical = TRUE)
sum(OutWheezing)
Find_outlier17 = which(OutWheezing == TRUE, arr.ind = TRUE)
OutWheezing
Find_outlier17
data = data[-Find_outlier17,]

OutSwallow = outlier(data$Swallowing.Difficulty, logical = TRUE)
sum(OutSwallow)
Find_outlier18 = which(OutSwallow == TRUE, arr.ind = TRUE)
OutSwallow
Find_outlier18
data = data[-Find_outlier18,]

OutClubbing = outlier(data$Clubbing.of.Finger.Nails, logical = TRUE)
sum(OutClubbing)
Find_outlier19 = which(OutClubbing == TRUE, arr.ind = TRUE)
OutClubbing
Find_outlier19
data = data[-Find_outlier19,]

OutCold = outlier(data$Frequent.Cold, logical = TRUE)
sum(OutCold)
Find_outlier20 = which(OutCold == TRUE, arr.ind = TRUE)
OutCold
Find_outlier20
data = data[-Find_outlier20,]

OutDryCough = data(CancerSet$Dry.Cough, logical = TRUE)
sum(OutDryCough)
Find_outlier21 = which(OutDryCough == TRUE, arr.ind = TRUE)
OutDryCough
Find_outlier21
data = data[-Find_outlier21,]

OutSnoring = outlier(data$Snoring, logical = TRUE)
sum(OutSnoring)
Find_outlier22 = which(OutSnoring == TRUE, arr.ind = TRUE)
OutSnoring
Find_outlier22
data = data[-Find_outlier22,]
```


```{r}
print(data)
### Printing the data after smoothing.
```


###################


## 10- Data transformation
### Ruaa 

#### Our dataset has clear data that doesn't need incoding other than converting the level fcator for level attribute, which we did in "7- statistical measurements".
#### Other than that, the data has balanced values and has no intervals that needs discretization, therefore, our data does not need any transformation.


## 11- Classification

```{r}
library(rpart.plot)
library(caret)
### Load libraries.
```

```{r}
features <- data[, c(-1,-24)]  
### Selecting all attributes except "Patient Id" and "Level".

target <- data$Level
### Choosing the class label "Level" as the target variable.

splits <- createDataPartition(target, p = 0.7, list = FALSE)
### Partitioning data based on the class label.
### We used 70% of the data as a training set.

str(data)
### Summary of the data after partitioning, it still has 24 variables but with 143 observations now.
```

```{r}
id3_tree <- train(
  x = features[splits, ],
  y = target[splits],
  method = "rpart",
  parms = list(split = "information"),
  trControl = trainControl(method = "cv", number = 3)  #### Adjust number for different folds.
)

### Initialized 3 decision tree models with different algorithms, splitting criterion is the information gain.


### Rinse and repeat to create 2 more trees:
id5_tree <- train(
  x = features[splits, ],
  y = target[splits],
  method = "rpart",
  parms = list(split = "information"),
  trControl = trainControl(method = "cv", number = 5)  
)

id10_tree <- train(
  x = features[splits, ],
  y = target[splits],
  method = "rpart",
  parms = list(split = "information"),
  trControl = trainControl(method = "cv", number = 10)
)
```

```{r}
print(id3_tree)
print(id5_tree)
print(id10_tree)
### Print or store the results of the 3 trees.
```
id3_rpart_model_3 <- id3_tree$finalModel
rpart.plot(id3_rpart_model_3)
id5_rpart_model_5 <- id5_tree$finalModel
rpart.plot(id5_rpart_model_5)
id10_rpart_model_10 <- id10_tree$finalModel
rpart.plot(id10_rpart_model_10)

### Initializing 3 more decision tree models with different algorithms, splitting criterion is gain ratio:
```{r}
c45_tree3 <- train(
  x = features[splits, ],
  y = target[splits],
  method = "rpart",
  parms = list(split = "gain"),
  trControl = trainControl(method = "cv", number = 3)  ### Adjust number for different folds.
)

c45_tree5 <- train(
  x = features[splits, ],
  y = target[splits],
  method = "rpart",
  parms = list(split = "gain"),
  trControl = trainControl(method = "cv", number = 5) 
)

c45_tree10 <- train(
  x = features[splits, ],
  y = target[splits],
  method = "rpart",
  parms = list(split = "gain"),
  trControl = trainControl(method = "cv", number = 10)
)

print(c45_tree3)
print(c45_tree5)
print(c45_tree10)
### Print or store the results of the 3 trees.
```
c45_rpart_model_3 <- c45_tree3$finalModel
rpart.plot(c45_rpart_model_3)
c45_rpart_model_5 <- c45_tree5$finalModel
rpart.plot(c45_rpart_model_5)
c45_rpart_model_10 <- c45_tree10$finalModel
rpart.plot(c45_rpart_model_10)

### Initializing 3 more decision tree models with different algorithms, splitting criterion is the gini index.

```{r}
cart_tree3 <- train(
  x = features[splits, ],
  y = target[splits],
  method = "rpart",
  parms = list(split = "gini"),
  trControl = trainControl(method = "cv", number = 3)  # Adjust number for different folds
)

cart_tree5 <- train(
  x = features[splits, ],
  y = target[splits],
  method = "rpart",
  parms = list(split = "gini"),
  trControl = trainControl(method = "cv", number = 3)  # Adjust number for different folds
)

cart_tree10 <- train(
  x = features[splits, ],
  y = target[splits],
  method = "rpart",
  parms = list(split = "gini"),
  trControl = trainControl(method = "cv", number = 3)  # Adjust number for different folds
)

print(cart_tree3)
print(cart_tree5)
print(cart_tree10)
### Print or store the results of the 3 trees.
```
cart_rpart_model_3 <- cart_tree3$finalModel
rpart.plot(cart_rpart_model_3)
cart_rpart_model_5 <- cart_tree5$finalModel
rpart.plot(cart_rpart_model_5)
cart_rpart_model_10 <- cart_tree10$finalModel
rpart.plot(cart_rpart_model_10)





###################################




## 12 - Clustering:

### By applying clustering *unsupervised learning*, we will partition our dataset into groups where the data in the same group are similar to each other and the data from different groups are dissimilar.

```{r}
library(factoextra)
library(cluster)
library(NbClust)
### Load libraries.
```

```{r}
new_data<- data[, c(-1,-24)]
### Create a new dataset without the "index" and "Patient Id" columns (irrelevant columns (uniqe)) and class attributes ( Level) to find similarities between data.
print(new_data)
### Print the new dataset
```



### Using k-means method:
```{r}
set.seed(8953)
### setting a seed for random number generation  to make the results reproducible.
```

```{r}
### Data types should be transformed into numeric types before clustering.
new_data <- scale(new_data)
View(new_data)
### All data is numeric.
```


### we chose three random numbers which are 2,3,5.
```{r}
kmeans.result <- kmeans(new_data, 2)
### run kmeans clustering to find 2 clusters.
kmeans.result
### print the clustering result.
### Within cluster sum of squares by cluster: 22.1%
```

```{r}
# Get the total within-cluster sum of squares (WCSS)
wcss <- sum(kmeans.result$tot.withinss)

# Print the WCSS
print(wcss)
#Within cluster sum of squares by cluster: 15093.78
```
### visualize clustering:
```{r}
fviz_cluster(kmeans.result, data = new_data) 
### it is good because there is no overlap between the clusters.

silhouette <- silhouette(kmeans.result$cluster,dist(new_data))
fviz_silhouette(silhouette)
### average for each cluster.
### k-means clustering with estimating k and initializations.
### The average silhouette width is 0.33, indicating reasonable clustering quality.
```

```{r}
### A higher precision indicates more accurate assignments within each cluster, while a higher recall indicates a better ability to capture the true cluster membership.
cluster_assignments <- c(kmeans.result$cluster)
ground_truth_labels <- c(data$Level)
data2 <- data.frame(cluster = cluster_assignments, label = ground_truth_labels)


## Function to calculate BCubed precision and recall:
calculate_bcubed_metrics <- function(data2) {
  n <- nrow(data2)
  precision_sum <- 0
  recall_sum <- 0
  
  for (i in 1:n) {
    cluster <- data2$cluster[i]
    label <- data2$label[i]
    
    ### Count the number of items from the same category within the same cluster:
    same_category_same_cluster <- sum(data2$label[data2$cluster == cluster] == label)
    
    ### Count the total number of items in the same cluster:
    total_same_cluster <- sum(data2$cluster == cluster)
    
    ### Count the total number of items with the same category:
    total_same_category <- sum(data2$label == label)
    
    ### Calculate precision and recall for the current item and add them to the sums:
    precision_sum <- precision_sum + same_category_same_cluster /total_same_cluster
    recall_sum <- recall_sum + same_category_same_cluster / total_same_category
  }
  
  ### Calculate average precision and recall:
  precision <- precision_sum / n
  recall <- recall_sum / n
  
  return(list(precision = precision, recall = recall))
}

### Calculate BCubed precision and recall:
metrics <- calculate_bcubed_metrics(data2)

### Extract precision and recall from the metrics:
precision <- metrics$precision
recall <- metrics$recall

### Print the results:
cat("BCubed Precision:", precision, "\n")
cat("BCubed Recall:", recall, "\n")
### The BCubed precision is 0.8942521. This indicates that the clustering results have a precision of approximately 0.9, meaning that around 90% of the data points within each cluster are correctly assigned.
### The BCubed recall is 0.6674904. This suggests that the clustering results have a recall of approximately 0.67, indicating that around 67% of the data points that should belong to the same cluster are correctly assigned to that cluster.

### (highest precision and recall values)
```

```{r}
kmeans.result <- kmeans(new_data, 3)
### Run kmeans clustering to find 3 clusters
kmeans.result
### print the clusterng result
### Within cluster sum of squares by cluster:38.3%.
```

### visualize clustering:
```{R}
fviz_cluster(kmeans.result, data = new_data) 
### visualization suggests that there is some overlap between clusters, indicating poorer clustering quality(worst)

## Cluster Validation:
### average for each cluster:
silhouette <- silhouette(kmeans.result$cluster,dist(new_data))
fviz_silhouette(silhouette)
### k-means clustering with estimating k and initializations 
### The average silhouette width is 0.39, which is lower than for k = 2.
```

```{r}
cluster_assignments <- c(kmeans.result$cluster)
ground_truth_labels <- c(data$Level)
data2 <- data.frame(cluster = cluster_assignments, label = ground_truth_labels)

### Function to calculate BCubed precision and recall:
calculate_bcubed_metrics <- function(data2) {
  n <- nrow(data2)
  precision_sum <- 0
  recall_sum <- 0
  
  for (i in 1:n) {
    cluster <- data2$cluster[i]
    label <- data2$label[i]
    
    ### Count the number of items from the same category within the same cluster:
    same_category_same_cluster <- sum(data2$label[data2$cluster == cluster] == label)
    
    ### Count the total number of items in the same cluster:
    total_same_cluster <- sum(data2$cluster == cluster)
    
    ### Count the total number of items with the same category:
    total_same_category <- sum(data2$label == label)
    
    ### Calculate precision and recall for the current item and add them to the sums:
    precision_sum <- precision_sum + same_category_same_cluster /total_same_cluster
    recall_sum <- recall_sum + same_category_same_cluster / total_same_category
  }
  
  ### Calculate average precision and recall:
  precision <- precision_sum / n
  recall <- recall_sum / n
  
  return(list(precision = precision, recall = recall))
}

### Calculate BCubed precision and recall:
metrics <- calculate_bcubed_metrics(data2)

### Extract precision and recall from the metrics:
precision <- metrics$precision
recall <- metrics$recall

### Print the results:
cat("BCubed Precision:", precision, "\n")
cat("BCubed Recall:", recall, "\n")
#The BCubed precision is 0.8942521. This indicates that the clustering results have a recall of approximately 0.9, meaning that around 90% of the data points within each cluster are correctly assigned.
#The BCubed recall is 0.4066986 This indicates that the clustering results have a recall of approximately 0.40, suggesting that around 40% of the data points that should belong to the same cluster are correctly assigned to that cluster.
```


```{r}
kmeans.result <- kmeans(new_data, 5)
### Run kmeans clustering to find 4 clusters.
kmeans.result
### Print the clustering result.
### Within cluster sum of squares by cluster:52.8%
```

### visualize clustering:
```{r}
fviz_cluster(kmeans.result, data = new_data)
### The visualization suggests that there is even more overlap between clusters, indicating further degradation in clustering quality (worst).


### Cluster Validation:

### average for each cluster:
silhouette <- silhouette(kmeans.result$cluster,dist(new_data))
fviz_silhouette(silhouette)
### k-means clustering with estimating k and initializations.
### The average silhouette width is 0.46, which is higher than for k = 2 and k = 3.
```
```{r}
cluster_assignments <- c(kmeans.result$cluster)
ground_truth_labels <- c(data$Level)
data2 <- data.frame(cluster = cluster_assignments, label = ground_truth_labels)

## Function to calculate BCubed precision and recall:
calculate_bcubed_metrics <- function(data2) {
  n <- nrow(data2)
  precision_sum <- 0
  recall_sum <- 0
  
  for (i in 1:n) {
    cluster <- data2$cluster[i]
    label <- data2$label[i]
    
    ### Count the number of items from the same category within the same cluster:
    same_category_same_cluster <- sum(data2$label[data2$cluster == cluster] == label)
    
    ### Count the total number of items in the same cluster:
    total_same_cluster <- sum(data2$cluster == cluster)
    
    ### Count the total number of items with the same category:
    total_same_category <- sum(data2$label == label)
    
    ### Calculate precision and recall for the current item and add them to the sums:
    precision_sum <- precision_sum + same_category_same_cluster /total_same_cluster
    recall_sum <- recall_sum + same_category_same_cluster / total_same_category
  }
  
  ### Calculate average precision and recall:
  precision <- precision_sum / n
  recall <- recall_sum / n
  
  return(list(precision = precision, recall = recall))
}

### Calculate BCubed precision and recall:
metrics <- calculate_bcubed_metrics(data2)

### Extract precision and recall from the metrics:
precision <- metrics$precision
recall <- metrics$recall

### Print the results:
cat("BCubed Precision:", precision, "\n")
cat("BCubed Recall:", recall, "\n")

### The BCubed precision is 0.8942521 This suggests that the clustering results have a precision of approximately 0.9, meaning that around 90% of the data points within each cluster are correctly assigned.
#The BCubed recall is 0.4066986 This implies that the clustering results have a recall of approximately 0.4, suggesting that around 40% of the data points that should belong to the same cluster are correctly assigned to that cluster.
```


## Optimal number of cluster for all clusters:
```{r}
### fviz_nbclust() with silhouette method using library(factoextra) 
fviz_nbclust(new_data, kmeans, method = "wss")+ labs(subtitle = "Elbow method")
### the best number of clusters and what will give us the best clusters of our data values is when k=5 from the Elbow method.
### k = 5 achieves the highest precision and recall values among the tested values of k.
```
